{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Json2Graph import json_to_graph\n",
    "from DrawGraph import draw_nodes, draw_edges, draw_labels, draw_graph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = json_to_graph('data/dataset_20180403/180402_Lukas_Herkunfts-_Pflegefamilie_Schule.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weighted Degree:')\n",
    "print(G.degree(weight='weight'))\n",
    "print('Positive Weighted Degree:')\n",
    "print(G.degree(weight='strengthen'))\n",
    "print('Negative Weighted Degree:')\n",
    "print(G.degree(weight='weaken'))\n",
    "print('Absolute Weighted Degree:')\n",
    "print(G.degree(weight='weight_absolute'))\n",
    "print('Degree:')\n",
    "print(G.degree())\n",
    "print('Weighted In-Degree:')\n",
    "print(G.in_degree(weight='weight'))\n",
    "print('Positive Weighted In-Degree:')\n",
    "print(G.in_degree(weight='strengthen'))\n",
    "print('Negative Weighted In-Degree:')\n",
    "print(G.in_degree(weight='weaken'))\n",
    "print('Absolute Weighted Degree:')\n",
    "print(G.in_degree(weight='weight_absolute'))\n",
    "print('In-Degree:')\n",
    "print(G.in_degree())\n",
    "print('Weighted Out-Degree:')\n",
    "print(G.out_degree(weight='weight'))\n",
    "print('Positive Weighted Out-Degree:')\n",
    "print(G.out_degree(weight='strengthen'))\n",
    "print('Negative Weighted Out-Degree:')\n",
    "print(G.out_degree(weight='weaken'))\n",
    "print('Absolute Weighted Degree:')\n",
    "print(G.out_degree(weight='weight_absolute'))\n",
    "print('Out-Degree:')\n",
    "print(G.out_degree(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = -2\n",
    "print(abs(w) if np.sign(w) == 1 else 0)\n",
    "print(abs(w) if np.sign(w) == -1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_edge_attributes(G):\n",
    "    all_attributes = []\n",
    "\n",
    "    for n, nbrsdict in G.adjacency():\n",
    "        for nbr, keydict in nbrsdict.items():\n",
    "            for key, eattr in keydict.items():\n",
    "                for attribute in eattr.keys():\n",
    "                    all_attributes.append(attribute)\n",
    "                pass\n",
    "\n",
    "    all_attributes = list(set(all_attributes))\n",
    "    \n",
    "    return all_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import approximation as ap\n",
    "print('Approximation:')\n",
    "print('all_pairs_node_connectivity:')\n",
    "apc = ap.all_pairs_node_connectivity(G)\n",
    "for key, value in apc.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('node_connectivity: {}'.format(ap.node_connectivity(G)))\n",
    "#ap.k_components(G) - only undirected\n",
    "print('max_clique: {}'.format(ap.max_clique(G)))\n",
    "#ap.average_clustering(G) - only undirected\n",
    "#ap.min_weighted_dominating_set(G, weight='weight') - only undirected\n",
    "print('maximum_independent_set: {}'.format(ap.maximum_independent_set(G)))\n",
    "print('min_maximal_matching: {}'.format(ap.min_maximal_matching(G)))\n",
    "print('ramsey_R2: {}'.format(ap.ramsey_R2(G)))\n",
    "#ap.metric_closure(G, weight='weight') - only undirected\n",
    "print('min_weighted_vertex_cover: {}'.format(ap.min_weighted_vertex_cover(G, weight='weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community as co\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "\n",
    "def heaviest(G):\n",
    "    u, v, w = max(G.edges(data='weight'), key=itemgetter(2))\n",
    "    return (u, v)\n",
    "\n",
    "k = 20\n",
    "comp = co.girvan_newman(G, most_valuable_edge=heaviest)\n",
    "for communities in itertools.islice(comp, k):\n",
    "    print(tuple(sorted(c) for c in communities)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_to_dataframe(df, name, func, weights=[]):\n",
    "    if len(weights) == 0:\n",
    "        data = func(G)\n",
    "        kwargs = {name : list(data.values())}\n",
    "        df = df.assign(**kwargs)\n",
    "    else:\n",
    "        for i, weight in enumerate(weights):\n",
    "            data = func(G, weight=weight)\n",
    "            kwargs = {'{} - {}'.format(name, weight) : list(data.values())}\n",
    "            df = df.assign(**kwargs)\n",
    "    return df\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df = add_to_dataframe(df, 'degree_centrality', nx.degree_centrality)\n",
    "df = add_to_dataframe(df, 'in_degree_centrality', nx.in_degree_centrality)\n",
    "df = add_to_dataframe(df, 'out_degree_centrality', nx.out_degree_centrality)\n",
    "df = add_to_dataframe(df, 'eigenvector_centrality_numpy', nx.eigenvector_centrality_numpy)\n",
    "df = add_to_dataframe(df, 'eigenvector_centrality_numpy', nx.eigenvector_centrality_numpy, ['weight','weight_absolute','strengthen','weaken'])\n",
    "df = add_to_dataframe(df, 'closeness_centrality', nx.degree_centrality)\n",
    "df = add_to_dataframe(df, 'betweenness_centrality', nx.betweenness_centrality)\n",
    "df = add_to_dataframe(df, 'betweenness_centrality', nx.betweenness_centrality, ['weight','weight_absolute','strengthen','weaken'])\n",
    "#df = add_to_dataframe(df, 'edge_betweenness_centrality', nx.edge_betweenness_centrality)\n",
    "#df = add_to_dataframe(df, 'edge_betweenness_centrality', nx.edge_betweenness_centrality, ['weight','weight_absolute','strengthen','weaken'])\n",
    "#df = add_to_dataframe(df, 'approximate_current_flow_betweenness_centrality', nx.approximate_current_flow_betweenness_centrality)\n",
    "#df = add_to_dataframe(df, 'approximate_current_flow_betweenness_centrality', nx.approximate_current_flow_betweenness_centrality, ['weight','weight_absolute','strengthen','weaken'])\n",
    "df = add_to_dataframe(df, 'load_centrality', nx.load_centrality)\n",
    "df = add_to_dataframe(df, 'load_centrality', nx.load_centrality, ['weight_absolute','strengthen','weaken'])\n",
    "#df = add_to_dataframe(df, 'edge_load_centrality', nx.edge_load_centrality)\n",
    "df = add_to_dataframe(df, 'harmonic_centrality', nx.harmonic_centrality)\n",
    "df = add_to_dataframe(df, 'betweenness_centrality', nx.betweenness_centrality, ['weight','weight_absolute','strengthen','weaken'])\n",
    "\n",
    "\n",
    "print('edge_betweenness_centrality: {}'.format(nx.edge_betweenness_centrality(G)))\n",
    "print('edge_betweenness_centrality (weighted): {}'.format(nx.edge_betweenness_centrality(G, weight='weight')))\n",
    "print('edge_load_centrality: {}'.format(nx.load_centrality(G)))\n",
    "print('global_reaching_centrality: {}'.format(nx.global_reaching_centrality(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplify_network(G, alpha=1.):\n",
    "    local_g = G.copy()\n",
    "    egde_traversal = list(nx.edge_dfs(local_g))\n",
    "    influence = nx.get_node_attributes(local_g, 'influence')\n",
    "    weight_absolute = nx.get_edge_attributes(local_g, 'weight_absolute')\n",
    "    \n",
    "    for i, (from_node, to_node, node_key) in enumerate(egde_traversal):\n",
    "#         print('traveling from {} to {}'.format(from_node, to_node))\n",
    "#         print('influence of {} before: {}'.format(to_node, influence[to_node]))\n",
    "#         print('influence[from_node]: {}'.format(influence[from_node]))\n",
    "#         print('influence[to_node]: {}'.format(influence[to_node]))\n",
    "        computed_influence = influence[to_node] + (influence[from_node] * (alpha * weight_absolute[(from_node, to_node, 0)]))\n",
    "#         print('computed_influence: {}'.format(computed_influence))\n",
    "        if influence[to_node] == 0:\n",
    "            influence[to_node] = 0\n",
    "        elif np.sign(influence[to_node]) == 1:\n",
    "            influence[to_node] = computed_influence if computed_influence > 0 else 0\n",
    "        else:\n",
    "            influence[to_node] = computed_influence if computed_influence < 0 else 0\n",
    "        print('influence of {} after: {}'.format(to_node, influence[to_node]))\n",
    "#         print()\n",
    "        \n",
    "    nx.set_node_attributes(local_g, influence,'influence')\n",
    "    \n",
    "    return local_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daG = amplify_network(G, alpha=0.05)\n",
    "print()\n",
    "for i in range(10):\n",
    "    daG = amplify_network(daG, alpha=0.05)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_metric(G, weights):\n",
    "    metric_data = {}\n",
    "    temp_data = {}\n",
    "    for i, weight in enumerate(weights):\n",
    "        temp_data['degree_{}'.format(weight)] = G.degree(weight=weight)\n",
    "        temp_data['in_degree_{}'.format(weight)] = G.in_degree(weight=weight)\n",
    "        temp_data['out_degree_{}'.format(weight)] = G.out_degree(weight=weight)\n",
    "    \n",
    "    temp_data['degree'] = G.degree()\n",
    "    temp_data['in_degree'] = G.in_degree()\n",
    "    temp_data['out_degree'] = G.out_degree()\n",
    "    \n",
    "    for i, metric in enumerate(temp_data):\n",
    "        metric_data[metric] = {}\n",
    "        for j, (node_id, value) in enumerate(temp_data[metric]):\n",
    "            metric_data[metric][node_id] = value\n",
    "    \n",
    "    return metric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_metric(G, ['weight','weight_absolute','strengthen','weaken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric(G, name, func, weights=[]):\n",
    "    metric_data = {}\n",
    "    if len(weights) == 0:\n",
    "        # Metric is unweighted\n",
    "        metric_data[name] = func(G)\n",
    "    else:\n",
    "        # Metric is weighted, go though every weight attribute\n",
    "        for i, weight in enumerate(weights):\n",
    "            metric_data['{}_{}'.format(name, weight)] = func(G, weight=weight)\n",
    "            \n",
    "    return metric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_metric(G, 'degree_centrality', nx.degree_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def graph_to_json(G):\n",
    "\n",
    "    json_object = {}\n",
    "    nodes = []\n",
    "    links = []\n",
    "    \n",
    "    # If a key does not exist on access, generate empty list\n",
    "    # So every node gets a 'cycle' attribute, even if it's just an empty list\n",
    "    cycles = defaultdict(list)\n",
    "    \n",
    "    # Get attributes from nodes (generated by JSON importer)\n",
    "    identifier = nx.get_node_attributes(G, name='id')\n",
    "    labels = nx.get_node_attributes(G, name='label')\n",
    "    action_systems_id = nx.get_node_attributes(G, name='actionSystemId')\n",
    "    action_systems = nx.get_node_attributes(G, name='actionSystem')\n",
    "    influences = nx.get_node_attributes(G, name='influence')\n",
    "    \n",
    "    # Get all possible directed graph metrics and add them to 'node_metrics' dict\n",
    "    node_metrics = {}\n",
    "    degree_metrics = rebuild_metric(G, ['weight','weight_absolute','strengthen','weaken'])\n",
    "    node_metrics.update(degree_metrics)\n",
    "    degree_centrality = generate_metric(G, 'degree_centrality', nx.degree_centrality)\n",
    "    node_metrics.update(degree_centrality)\n",
    "    in_degree_centrality = generate_metric(G, 'in_degree_centrality', nx.in_degree_centrality)\n",
    "    node_metrics.update(in_degree_centrality)\n",
    "    out_degree_centrality = generate_metric(G, 'out_degree_centrality', nx.out_degree_centrality)\n",
    "    node_metrics.update(out_degree_centrality)\n",
    "    eigenvector_centrality_numpy = generate_metric(G, 'eigenvector_centrality_numpy', nx.eigenvector_centrality_numpy)\n",
    "    node_metrics.update(eigenvector_centrality_numpy)\n",
    "    eigenvector_centrality_numpy_weighted = generate_metric(G, 'eigenvector_centrality_numpy', nx.eigenvector_centrality_numpy, ['weight','weight_absolute','strengthen','weaken'])\n",
    "    node_metrics.update(eigenvector_centrality_numpy_weighted)\n",
    "    closeness_centrality = generate_metric(G, 'closeness_centrality', nx.degree_centrality)\n",
    "    node_metrics.update(closeness_centrality)\n",
    "    betweenness_centrality = generate_metric(G, 'betweenness_centrality', nx.betweenness_centrality)\n",
    "    node_metrics.update(betweenness_centrality)\n",
    "    betweenness_centrality_weighted = generate_metric(G, 'betweenness_centrality', nx.betweenness_centrality, ['weight','weight_absolute','strengthen','weaken'])\n",
    "    node_metrics.update(betweenness_centrality_weighted)\n",
    "    load_centrality = generate_metric(G, 'load_centrality', nx.load_centrality)\n",
    "    node_metrics.update(load_centrality)\n",
    "    load_centrality_weighted = generate_metric(G, 'load_centralityload_centrality', nx.load_centrality, ['weight_absolute','strengthen','weaken'])\n",
    "    node_metrics.update(load_centrality_weighted)\n",
    "    harmonic_centrality = generate_metric(G, 'harmonic_centrality', nx.harmonic_centrality)\n",
    "    node_metrics.update(harmonic_centrality)\n",
    "    betweenness_centrality_weighted = generate_metric(G, 'betweenness_centrality', nx.betweenness_centrality, ['weight','weight_absolute','strengthen','weaken'])\n",
    "    node_metrics.update(betweenness_centrality_weighted)\n",
    "    \n",
    "    # Find cycles and build a 'cycle id' list\n",
    "    # Nodes with the same 'cycle id' belong to the same cycle\n",
    "    for i, cycle in enumerate(list(nx.simple_cycles(G))):\n",
    "        for j, node in enumerate(cycle):\n",
    "            cycles[node].append(i)\n",
    "            \n",
    "    # Generate sorted int list of 'cycle id'\n",
    "    cycles_list = [int(i) for i in sorted(list(cycles.keys()))]\n",
    "    \n",
    "    # Fill 'nodes' dict\n",
    "    for i in range(len(identifier)):\n",
    "        attributes = {\n",
    "            'id': identifier[i],\n",
    "            'label': labels[i],\n",
    "            'influence': influences[i],\n",
    "            'actionSystemId': action_systems_id[i],\n",
    "            'actionSystem': action_systems[i],\n",
    "            'cycles': cycles[i]\n",
    "        }\n",
    "        \n",
    "        for k, metric in enumerate(node_metrics):\n",
    "            attributes[metric] = node_metrics[metric][i]\n",
    "        \n",
    "        nodes.append(attributes)\n",
    "\n",
    "    # Get attributes from edges (generated by JSON importer)\n",
    "    weight_absolute = nx.get_edge_attributes(G, name='weight_absolute')\n",
    "    strengthen = nx.get_edge_attributes(G, name='strengthen')\n",
    "    weaken = nx.get_edge_attributes(G, name='weaken')\n",
    "    sign = nx.get_edge_attributes(G, name='sign')\n",
    "    \n",
    "    # Get all possible directed graph metrics and add them to 'edge_metrics' dict\n",
    "    edge_metrics = {}\n",
    "    edge_betweenness_centrality = generate_metric(G, 'edge_betweenness_centrality', nx.edge_betweenness_centrality)\n",
    "    edge_metrics.update(edge_betweenness_centrality)\n",
    "    edge_betweenness_centrality_weighted = generate_metric(G, 'edge_betweenness_centrality', nx.edge_betweenness_centrality, weights=['weight','weight_absolute','strengthen','weaken'])\n",
    "    edge_metrics.update(edge_betweenness_centrality_weighted)\n",
    "    edge_load_centrality = generate_metric(G, 'edge_load_centrality', nx.edge_load_centrality)\n",
    "    edge_metrics.update(edge_load_centrality)\n",
    "    \n",
    "    # Fill 'links' dict\n",
    "    for (from_node, to_node, weight) in G.edges(data='weight'):\n",
    "        attributes = {\n",
    "            'source': nodes[from_node]['id'],\n",
    "            'target': nodes[to_node]['id'],\n",
    "            'weight': weight,\n",
    "            'weight_absolute': weight_absolute[(from_node, to_node, 0)],\n",
    "            'strengthen': strengthen[(from_node, to_node, 0)],\n",
    "            'weaken': weaken[(from_node, to_node, 0)],\n",
    "            'sign': sign[(from_node, to_node, 0)]\n",
    "        }\n",
    "        \n",
    "        for k, metric in enumerate(edge_metrics):\n",
    "            attributes[metric] = edge_metrics[metric][(from_node, to_node)]\n",
    "            \n",
    "        links.append(attributes)\n",
    "    \n",
    "    json_object['cycles'] = cycles_list\n",
    "    json_object['nodes'] = nodes\n",
    "    json_object['links'] = links\n",
    "    \n",
    "    return json.JSONEncoder(ensure_ascii=False, allow_nan=False).encode(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def graph_to_json_file(G, filename):\n",
    "    file = codecs.open(filename, \"w\", \"utf-8-sig\")\n",
    "    file.write(graph_to_json(G))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_to_json_file(G, \"json-test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
